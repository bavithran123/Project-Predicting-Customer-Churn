# -*- coding: utf-8 -*-
"""Phase_2_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBBLgxnMtKa6IxQYpnop4rEScohu0X_K

Upload the Dataset
"""

from google.colab import files
uploaded = files.upload()

"""Load the Dataset"""

import pandas as pd
# Read the dataset
df = pd.read_csv('Churn_Modelling.csv')

"""Data Exploration"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Encode categorical variables for exploration
le_geo = LabelEncoder()
le_gender = LabelEncoder()
df['Geography'] = le_geo.fit_transform(df['Geography'])
df['Gender'] = le_gender.fit_transform(df['Gender'])

# 1. Basic Info
print("Data Summary:")
print(df.describe())
print("\nMissing values:\n", df.isnull().sum())

# 2. Target variable distribution
sns.countplot(x='Exited', data=df)
plt.title('Churn Distribution')
plt.xlabel('Exited (0=No, 1=Yes)')
plt.ylabel('Number of Customers')
plt.tight_layout()
plt.show()

# 3. Churn by Gender
sns.countplot(x='Gender', hue='Exited', data=df)
plt.title('Churn by Gender (0=Male, 1=Female)')
plt.xlabel('Gender')
plt.tight_layout()
plt.show()

# 4. Churn by Geography
sns.countplot(x='Geography', hue='Exited', data=df)
plt.title('Churn by Geography (Encoded)')
plt.xlabel('Geography')
plt.tight_layout()
plt.show()

#

"""Check for Missing Values and Duplicates"""

import pandas as pd

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')  # Update path if needed

# Check for missing values
print("ðŸ” Missing Values per Column:\n")
print(df.isnull().sum())

# Check for duplicates
duplicate_count = df.duplicated().sum()
print(f"\nðŸ§© Number of Duplicate Rows: {duplicate_count}")

"""Visualize a Few Features"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')  # Update path if necessary

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Set plot style
sns.set(style="whitegrid")

# 1. Churn Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Exited', data=df)
plt.title('Customer Churn Distribution')
plt.xlabel('Exited (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 2. Churn by Gender
plt.figure(figsize=(6, 4))
sns.countplot(x='Gender', hue='Exited', data=df)
plt.title('Churn by Gender')
plt.tight_layout()
plt.show()

# 3. Age Distribution
plt.figure(figsize=(8, 4))
sns.histplot(df['Age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.tight_layout()
plt.show()

# 4. Balance vs Churn
plt.figure(figsize=(8, 4))
sns.boxplot(x='Exited', y='Balance', data=df)
plt.title('Customer Balance vs Churn')
plt.tight_layout()
plt.show()

# 5. Tenure Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Tenure', data=df, palette='viridis')
plt.title('Tenure Distribution')
plt.tight_layout()
plt.show()

"""Identify Target and Features"""

import pandas as pd

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')  # Update path if needed

# Drop non-feature columns (IDs and names)
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Define target and features
target = 'Exited'
features = [col for col in df.columns if col != target]

# Output
print(f"ðŸŽ¯ Target Variable: {target}\n")
print("ðŸ“Œ Feature Columns:")
for col in features:
    print(f"- {col}")

"""Convert Categorical Columns to Numerical"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("ðŸ”¤ Categorical Columns to Encode:", list(categorical_cols))

# Initialize LabelEncoder
le = LabelEncoder()

# Apply Label Encoding to each categorical column
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Preview transformed data
print("\nâœ… Transformed Data (First 5 Rows):")
print(df.head())

"""One-Hot Encoding"""

import pandas as pd

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop non-feature columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Display original categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("ðŸ”¤ Categorical Columns:", list(categorical_cols))

# Apply one-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Display encoded columns
print("\nâœ… One-Hot Encoded Data (First 5 Rows):")
print(df_encoded.head())

# Display new shape of the dataset
print(f"\nðŸ“ New Shape: {df_encoded.shape}")

"""Feature Scaling"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-Hot Encode categorical variables
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Apply StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for inspection
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Preview scaled data
print("âœ… Feature Scaling Complete (First 5 Rows):")
print(X_scaled_df.head())

"""Train-Test Split"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and prepare dataset
df = pd.read_csv('Churn_Modelling.csv')
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-Hot Encode categorical variables
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-Test Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Output the shapes
print("âœ… Train-Test Split Complete:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape:  {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape:  {y_test.shape}")

"""Model Building"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load and preprocess the dataset
df = pd.read_csv('Churn_Modelling.csv')
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-hot encode categorical columns
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Build logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("âœ… Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""Evaluation"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Basic Evaluation
print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
print("\nðŸ“„ Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nðŸ”² Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""Make Prediction From New Input"""

import numpy as np

# ðŸ”¢ Example new customer input
# Order of features must match training data
new_customer = {
    'CreditScore': 600,
    'Age': 40,
    'Tenure': 3,
    'Balance': 60000.0,
    'NumOfProducts': 2,
    'HasCrCard': 1,
    'IsActiveMember': 1,
    'EstimatedSalary': 50000.0,
    'Geography_Germany': 1,
    'Geography_Spain': 0,
    'Gender_Male': 1
}

# Convert to DataFrame
input_df = pd.DataFrame([new_customer])

# Scale input data using the same scaler as training
input_scaled = scaler.transform(input_df)

# Make prediction
prediction = model.predict(input_scaled)[0]
result = "Churn" if prediction == 1 else "No Churn"

print(f"ðŸ“Š Prediction Result: {result}")

"""Convert to DataFrame and Encode"""

import pandas as pd

# New input as dictionary
new_customer = {
    'CreditScore': 600,
    'Geography': 'Germany',
    'Gender': 'Male',
    'Age': 40,
    'Tenure': 3,
    'Balance': 60000.0,
    'NumOfProducts': 2,
    'HasCrCard': 1,
    'IsActiveMember': 1,
    'EstimatedSalary': 50000.0
}

# Convert to DataFrame
new_df = pd.DataFrame([new_customer])

# One-hot encode (match training columns)
new_df_encoded = pd.get_dummies(new_df, drop_first=True)

print("âœ… Encoded New Input:")
print(new_df_encoded)

"""Deployment-Building an interactive App"""

import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression


# ... (previous code to train the model and scaler, as in ipython-input-16-da40d04dde0d) ...

# Save the model and scaler
with open('model.pkl', 'wb') as f:  # 'wb' for writing in binary mode
    pickle.dump(model, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("\nðŸ§  Customer Churn Prediction App\n")

# ... (rest of the code to collect user input and make predictions) ...

"""Create a Prediction Function"""

import pandas as pd
import pickle

# Load model and scaler once
model = pickle.load(open('model.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))

def predict_churn(credit_score, age, tenure, balance, num_products,
                  has_cr_card, is_active, estimated_salary,
                  geography, gender):
    """
    Predict whether a customer will churn based on input features.

    Returns:
        1 -> churn, 0 -> not churn
    """
    # Prepare feature dictionary
    input_data = {
        'CreditScore': credit_score,
        'Age': age,
        'Tenure': tenure,
        'Balance': balance,
        'NumOfProducts': num_products,
        'HasCrCard': has_cr_card,
        'IsActiveMember': is_active,
        'EstimatedSalary': estimated_salary,
        'Geography_Germany': 1 if geography == 'Germany' else 0,
        'Geography_Spain': 1 if geography == 'Spain' else 0,
        'Gender_Male': 1 if gender == 'Male' else 0
    }

    # Convert to DataFrame and scale
    input_df = pd.DataFrame([input_data])
    input_scaled = scaler.transform(input_df)

    # Predict and return result
    prediction = model.predict(input_scaled)[0]
    return int(prediction)

pip install gradio

"""Create the Gradio Interface"""

# ... (all the previous cells for data loading, preprocessing, model training, etc.) ...

# %%
# Install Gradio
!pip install gradio

# %%
# Now you can import and use Gradio
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
import gradio as gr # This import will now work

# Load the dataset
df = pd.read_csv("Churn_Modelling.csv")

# Prepare the features and target
X = df.drop(columns=["RowNumber", "CustomerId", "Surname", "Exited"])
y = df["Exited"]

# Define categorical and numerical columns
categorical_cols = ["Geography", "Gender"]
numerical_cols = [col for col in X.columns if col not in categorical_cols]

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numerical_cols),
        ("cat", OneHotEncoder(), categorical_cols),
    ]
)

# Build a model pipeline
model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(n_estimators=100, random_state=42))
])

# Split and train the model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model.fit(X_train, y_train)

# Define prediction function
def predict_churn(CreditScore, Geography, Gender, Age, Tenure, Balance,
                  NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary):
    input_data = pd.DataFrame([[
        CreditScore, Geography, Gender, Age, Tenure, Balance,
        NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary
    ]], columns=X.columns)
    prediction = model.predict(input_data)[0]
    probability = model.predict_proba(input_data)[0][1]
    return {
        "Churn": float(probability),
        "Stay": float(1 - probability)
    }

# Create Gradio UI
interface = gr.Interface(
    fn=predict_churn,
    inputs=[
        gr.Slider(300, 900, step=1, label="Credit Score"),
        gr.Dropdown(choices=["France", "Spain", "Germany"], label="Geography"),
        gr.Dropdown(choices=["Male", "Female"], label="Gender"),
        gr.Slider(18, 100, step=1, label="Age"),
        gr.Slider(0, 10, step=1, label="Tenure"),
        gr.Slider(0, 250000, step=100, label="Balance"),
        gr.Slider(1, 4, step=1, label="Number of Products"),
        gr.Radio(choices=[0, 1], label="Has Credit Card"),
        gr.Radio(choices=[0, 1], label="Is Active Member"),
        gr.Slider(0, 200000, step=100, label="Estimated Salary")
    ],
    outputs=gr.Label(num_top_classes=2),
    title="Customer Churn Predictor",
    description="Predict whether a bank customer is likely to churn based on personal and account data."
)

# Launch the app
interface.launch()