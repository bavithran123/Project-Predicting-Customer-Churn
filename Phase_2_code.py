# -*- coding: utf-8 -*-
"""Phase_2_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBBLgxnMtKa6IxQYpnop4rEScohu0X_K
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
# Read the dataset
df = pd.read_csv('Churn_Modelling.csv')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Encode categorical variables for exploration
le_geo = LabelEncoder()
le_gender = LabelEncoder()
df['Geography'] = le_geo.fit_transform(df['Geography'])
df['Gender'] = le_gender.fit_transform(df['Gender'])

# 1. Basic Info
print("Data Summary:")
print(df.describe())
print("\nMissing values:\n", df.isnull().sum())

# 2. Target variable distribution
sns.countplot(x='Exited', data=df)
plt.title('Churn Distribution')
plt.xlabel('Exited (0=No, 1=Yes)')
plt.ylabel('Number of Customers')
plt.tight_layout()
plt.show()

# 3. Churn by Gender
sns.countplot(x='Gender', hue='Exited', data=df)
plt.title('Churn by Gender (0=Male, 1=Female)')
plt.xlabel('Gender')
plt.tight_layout()
plt.show()

# 4. Churn by Geography
sns.countplot(x='Geography', hue='Exited', data=df)
plt.title('Churn by Geography (Encoded)')
plt.xlabel('Geography')
plt.tight_layout()
plt.show()

#

import pandas as pd

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')  # Update path if needed

# Check for missing values
print("ðŸ” Missing Values per Column:\n")
print(df.isnull().sum())

# Check for duplicates
duplicate_count = df.duplicated().sum()
print(f"\nðŸ§© Number of Duplicate Rows: {duplicate_count}")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')  # Update path if necessary

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Set plot style
sns.set(style="whitegrid")

# 1. Churn Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Exited', data=df)
plt.title('Customer Churn Distribution')
plt.xlabel('Exited (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 2. Churn by Gender
plt.figure(figsize=(6, 4))
sns.countplot(x='Gender', hue='Exited', data=df)
plt.title('Churn by Gender')
plt.tight_layout()
plt.show()

# 3. Age Distribution
plt.figure(figsize=(8, 4))
sns.histplot(df['Age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.tight_layout()
plt.show()

# 4. Balance vs Churn
plt.figure(figsize=(8, 4))
sns.boxplot(x='Exited', y='Balance', data=df)
plt.title('Customer Balance vs Churn')
plt.tight_layout()
plt.show()

# 5. Tenure Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Tenure', data=df, palette='viridis')
plt.title('Tenure Distribution')
plt.tight_layout()
plt.show()

import pandas as pd

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')  # Update path if needed

# Drop non-feature columns (IDs and names)
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Define target and features
target = 'Exited'
features = [col for col in df.columns if col != target]

# Output
print(f"ðŸŽ¯ Target Variable: {target}\n")
print("ðŸ“Œ Feature Columns:")
for col in features:
    print(f"- {col}")

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("ðŸ”¤ Categorical Columns to Encode:", list(categorical_cols))

# Initialize LabelEncoder
le = LabelEncoder()

# Apply Label Encoding to each categorical column
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Preview transformed data
print("\nâœ… Transformed Data (First 5 Rows):")
print(df.head())

import pandas as pd

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop non-feature columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Display original categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("ðŸ”¤ Categorical Columns:", list(categorical_cols))

# Apply one-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Display encoded columns
print("\nâœ… One-Hot Encoded Data (First 5 Rows):")
print(df_encoded.head())

# Display new shape of the dataset
print(f"\nðŸ“ New Shape: {df_encoded.shape}")

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-Hot Encode categorical variables
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Apply StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for inspection
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Preview scaled data
print("âœ… Feature Scaling Complete (First 5 Rows):")
print(X_scaled_df.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and prepare dataset
df = pd.read_csv('Churn_Modelling.csv')
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-Hot Encode categorical variables
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-Test Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Output the shapes
print("âœ… Train-Test Split Complete:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape:  {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape:  {y_test.shape}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load and preprocess the dataset
df = pd.read_csv('Churn_Modelling.csv')
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-hot encode categorical columns
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Build logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("âœ… Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Basic Evaluation
print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
print("\nðŸ“„ Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nðŸ”² Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import numpy as np

# ðŸ”¢ Example new customer input
# Order of features must match training data
new_customer = {
    'CreditScore': 600,
    'Age': 40,
    'Tenure': 3,
    'Balance': 60000.0,
    'NumOfProducts': 2,
    'HasCrCard': 1,
    'IsActiveMember': 1,
    'EstimatedSalary': 50000.0,
    'Geography_Germany': 1,
    'Geography_Spain': 0,
    'Gender_Male': 1
}

# Convert to DataFrame
input_df = pd.DataFrame([new_customer])

# Scale input data using the same scaler as training
input_scaled = scaler.transform(input_df)

# Make prediction
prediction = model.predict(input_scaled)[0]
result = "Churn" if prediction == 1 else "No Churn"

print(f"ðŸ“Š Prediction Result: {result}")

import pandas as pd

# New input as dictionary
new_customer = {
    'CreditScore': 600,
    'Geography': 'Germany',
    'Gender': 'Male',
    'Age': 40,
    'Tenure': 3,
    'Balance': 60000.0,
    'NumOfProducts': 2,
    'HasCrCard': 1,
    'IsActiveMember': 1,
    'EstimatedSalary': 50000.0
}

# Convert to DataFrame
new_df = pd.DataFrame([new_customer])

# One-hot encode (match training columns)
new_df_encoded = pd.get_dummies(new_df, drop_first=True)

print("âœ… Encoded New Input:")
print(new_df_encoded)

import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression


# ... (previous code to train the model and scaler, as in ipython-input-16-da40d04dde0d) ...

# Save the model and scaler
with open('model.pkl', 'wb') as f:  # 'wb' for writing in binary mode
    pickle.dump(model, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("\nðŸ§  Customer Churn Prediction App\n")

# ... (rest of the code to collect user input and make predictions) ...

import pandas as pd
import pickle

# Load model and scaler once
model = pickle.load(open('model.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))

def predict_churn(credit_score, age, tenure, balance, num_products,
                  has_cr_card, is_active, estimated_salary,
                  geography, gender):
    """
    Predict whether a customer will churn based on input features.

    Returns:
        1 -> churn, 0 -> not churn
    """
    # Prepare feature dictionary
    input_data = {
        'CreditScore': credit_score,
        'Age': age,
        'Tenure': tenure,
        'Balance': balance,
        'NumOfProducts': num_products,
        'HasCrCard': has_cr_card,
        'IsActiveMember': is_active,
        'EstimatedSalary': estimated_salary,
        'Geography_Germany': 1 if geography == 'Germany' else 0,
        'Geography_Spain': 1 if geography == 'Spain' else 0,
        'Gender_Male': 1 if gender == 'Male' else 0
    }

    # Convert to DataFrame and scale
    input_df = pd.DataFrame([input_data])
    input_scaled = scaler.transform(input_df)

    # Predict and return result
    prediction = model.predict(input_scaled)[0]
    return int(prediction)

pip install gradio

import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression


# Load and preprocess the dataset
df = pd.read_csv('Churn_Modelling.csv')
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-hot encode categorical columns
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Build logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Save the model and scaler
with open('model.pkl', 'wb') as f:  # 'wb' for writing in binary mode
    pickle.dump(model, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# ... (rest of the code to collect user input and make predictions) ...